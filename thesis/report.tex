%%
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[width=3cm]{sample.eps}
%\psfig{file=sample.eps,scale=0.6}
%\epsfile{file=sample.eps,scale=0.6}
%\end{center}
%\caption{図の例}
%\label{figure:sample}
%\end{figure}


% このファイルは、筑波大学情報学群情報科学類の
% 卒業研究論文本体のサンプルです。
% このファイルを書き換えて、この例と同じような書式の論文本体を
% LaTeXを使って作成することができます。
% 
% PC環境や、LaTeX環境の設定によっては漢字コードや改行コードを
% 変更する必要があります。
%%
\documentclass[a4paper,11pt]{jreport}

%%【PostScript, JPEG, PNG等の画像の貼り込み】
%% 利用するパッケージを選んでコメントアウトしてください。
\usepackage[dvipdfmx]{graphicx}  
%for includegraphics[width=3cm]{sample.eps}
%\usepackage{epsfig} % for \psfig{file=sample.eps,width=3cm}
%\usepackage{epsf} % for \epsfile{file=sample.eps,scale=0.6}
%\usepackage{epsbox} % for \epsfile{file=sample.eps,scale=0.6}
% MATH
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{varioref}
\usepackage[shortlabels]{enumitem}



\DeclareMathOperator{\cut}{cut\,}
\DeclareMathOperator{\spn}{span\,}
\DeclareMathOperator*{\vol}{vol\,}
\DeclareMathOperator*{\ncut}{Ncut\,}
\DeclareMathOperator*{\argmin}{\arg\!\min}
%\DeclareMathOperator*{\dim}{dim}


%% dvipdfm を使う場合(dvi->pdfを直接生成する場合)
%\usepackage[dvipdfm]{color,graphicx}
%% dvipdfm を使ってPDFの「しおり」を付ける場合
%\usepackage[dvipdfm,bookmarks=true,bookmarksnumbered=true,bookmarkstype=toc]{hyperref}
%% 参考：dvipdfm 日本語版
%% http://hamilcar.phys.kyushu-u.ac.jp/~hirata/dvipdfm/

\usepackage[left=25truemm,top=35truemm,right=25truemm,bottom=50truemm]{geometry}
\usepackage{times} % use Times Font instead of Computer Modern

\setcounter{tocdepth}{3}
\setcounter{page}{-1}

\setlength{\parskip}{0em}
\setlength{\topsep}{0em}

%\newcommand{\zu}[1]{{\gt \bf 図\ref{#1}}}

%% タイトル生成用パッケージ(重要)
\usepackage{coins-jp-utf8}

%% タイトル
%% 【注意】タイトルの最後に\\ を入れるとエラーになります
\title{\underline{Extending the use of the Bethe Hessian} \underline{to Constrained Spectral Clustering}}
%% 著者
\author{Soares Oliveira, Farley}
%% 指導教員
\advisor{櫻井鉄也}

%% 専攻名 と 年月 (提出年月)
%% 年月は必要に応じて書き替えてください。
\heiseiyear{29}  % 平成の年度
\majorfield{ソフトウェアサイエンス主専攻}
%\majorfield{情報システム主専攻}
%\majorfield{知能情報メディア主専攻}
\usepackage[english]{babel}
\renewcommand{\prechaptername}{Chap. }
\renewcommand{\postchaptername}{}




% NORM
\newcommand\norm[1]{\left\lVert#1\right\rVert}


% Integer Intervals
\usepackage{mathtools, stmaryrd}
\usepackage{xparse} \DeclarePairedDelimiterX{\Iintv}[1]{\llbracket}{\rrbracket}{\iintvargs{#1}}
\NewDocumentCommand{\iintvargs}{>{\SplitArgument{1}{,}}m}
{\iintvargsaux#1} %
\NewDocumentCommand{\iintvargsaux}{mm} {#1\mkern1.5mu, \mkern1.5mu#2}

% Algorithms
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

% THEOREM NADO

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}


\begin{document}
\maketitle
\thispagestyle{empty}
\newpage

\thispagestyle{empty}
\vspace*{20pt plus 1fil}
\parindent=1zw
\noindent
%%
%% 論文の概要(Abstract)
%%
\begin{center}
{\Large \bf Abstract}
\vspace{2cm}
\end{center}
Here you write the abstract of your thesis.

%%%%%
\par
\vspace{0pt plus 1fil}
\newpage

\pagenumbering{roman} % I, II, III, IV 
\tableofcontents
\listoffigures
%\listoftables

\pagebreak \setcounter{page}{1}
\pagenumbering{arabic} % 1,2,3


\chapter{Introduction}


\section{Notation}

\chapter{Spectral Clustering}
%%%% The reader may skip this section (...). %%%%
In this chapter we define the clustering problem, describe general ways in which it can be solved, and introduce
spectral clustering as a solution to this problem which uses the Courant-Fischer Min-Max Theorem.
The material here is based mainly on \cite{ng} and \cite{tutorial}.
However, we have changed the notations and some of the presentation, selecting only the relevant parts for the rest of this thesis.
We have also provided several proofs which were omitted in the original papers.
Readers who are already familiar with the derivation of spectral clustering may feel free to skip this chapter. 

\section{The clustering problem}

% How is the clustering problem defined?
Clustering is currently the most popular way of conducting unsupervised learning. 
Given a dataset $\mathcal D$, the objective of clustering is to find a proper partition of $\mathcal D$, $(\mathcal P_1, \mathcal P_2, \cdots, \mathcal P_k)$, where $k \in \mathbb Z_{>1}$ is predetermined by the user of the algorithm, such that the similarity of elements of a same subset $\mathcal P_i$  $(\text{with } i \in \Iintv{1,k})$ are as big as possible and the similarity of elements of different subsets $\mathcal P_i$ and $\mathcal P_j$ $(\text{where } (i,j) \in \Iintv{1,k}^2, i \ne j)$ are as small as possible. 
In other words, a clustering algorithm assigns a label $l \in \Iintv{1,k}$ to each data instance in $\mathcal D$ in such a way that data instances which are similar to each other are assigned the same label.
The way the similarity of elements of a same subset and the similarity of elements of different subsets are calculated depends on the clustering algorithm used. 
We can say the same about the way in which the dataset $\mathcal D$ is represented.

% What are some non-spectral ways of doing clustering?
Clustering may be achieved by several different approaches, each with its own advantages and disadvantages.
Some models and approaches for clustering are:
\begin{itemize}
   \item Strict partioning clustering: each data instance is classified into one cluster based on its similarity with other instances.
      The main approach for this type of clustering is k-means: the algorithm works by iteratively assigning a label to each data instance based on its similarity with each cluster.
      Here, the similarity of a data instance and a cluster is obtained by computing the similarity between the instance and some kind of representative data instance of the cluster, usually some kind of mean vector.
   \item Hierarchical clustering: the data is divided in clusters which make up a hierarchy.
      This type of clustering may be achieved by two main approaches: the agglomerative approach, where each data instance starts in its own cluster, and pairs of clusters are merged as we go up in the hierarchy; and the divisive approach, where all data instances start in a same cluster, and clusters are split as we go down the hierarchy.
      One advantage of hierarchical clustering is that the algorithm user does not need to set the number of subsets $k$ ahead of time.
   \item Overlapping clustering: In the final result, each data instance may be an element of more than one cluster.
      In other words, $(\mathcal P_1, \mathcal P_2, \cdots, \mathcal P_k)$ is not necessarily a partition of $\mathcal D$.
      This approach may be useful when certain data instances naturally pertain to more than one class.
\end{itemize}

% Spectral Clustering? What are the characteristics (advantages) of a spectral approach?
In contrast to the approaches above, spectral clustering works by transforming the data into a graph, constructing a certain matrix associated to this graph called the Laplacian, computing the eigenvalues and eigenvectors of the Laplacian, and finally using this eigeninformation to classify the data.
Although spectral clustering is often more difficult to implement (requiring, e.g., an algorithm to efficiently solve an eigenproblem), it is more general than the more common approaches such as k-means and hierarchial clustering.
This is because spectral clustering may be succesfuly used for data that are arranged in complex shapes (as long as each cluster is connected), since the data is first mapped from their native data space to another one in which connectivity is preserved but geometrical relationships are simplified.

% What are the two forms spectral clustering can be derived?
There are two main approaches with which spectral clustering can be derived.
The first approach, the \textit{ideal case} approach, considers regular Laplacian matrices as perturbations of an ideal case in which data points that are to be classified into different clusters are infinitely far apart.
The second approach, the \textit{relaxation} approach, considers spectral clustering as a approximation algorithm to solve a original NP-complete discrete optimization problem.
The former is related to the Bethe Hessia spectral clustering algorithm, while the latter is related to the FAST-GE-2.0 algorithm, both of which will be discussed henceforth in this thesis.
For this reason, we will explain both approaches in this chapter.

We show the Spectral Clustering Algorithm below and explain why it outputs a valid result in the subsequent sections.

\begin{algorithm}
\caption{Spectral Clustering}\label{spectral_clustering}
\begin{algorithmic}[1]
   \Require 
      \Statex Adjacency Matrix of the graph $G = (V,E)$: $A \in \mathbb R ^ {m \times m}$ 
      \Statex Number of Clusters: $k \in \mathbb Z_{>1}$
   \Ensure 
      \Statex Partition of the set of vertices $V$: $\{ C_1, C_2, \cdots, C_k \} \subseteq V$
      \vspace{0.2 cm}
   \State Compute the normalized Laplacian $L$ of $A$.
   \State Compute the first $k$ eigenvectors $(x_1, x_2, \cdots, x_k) \in (\mathbb R^{m})^k$ of $L$.
   \State Let $X \in \mathbb R^{m \times k}$ be the matrix containing the vectors $x_1, x_2, \cdots, x_k$ as columns.
   \State Form the matrix $Y \in \mathbb R^{m \times k}$ by normalizing the columns of $X$.
   \State Let $(y_1, y_2, \cdots, y_m) \in ( \mathbb R^{1 \times k} )^m$ represent the row-vectors of $Y$.
   \State Cluster $(y_1, y_2, \cdots, y_m)$ using $k$-means into clusters $\{ D_1, D_2, \cdots, D_k \} $.
   \State For each $i \in \Iintv{1,k}$, set $C_i = \{ v_j \in V: y_j \in D_i \}$.
\end{algorithmic}
\end{algorithm}




\section{Preliminary definitions} 
Before entering the discussion of each derivation, we will give some definitions common to both approaches.
Here we will assume that each data instance $d \in \mathcal D$ is a $n$ dimensional column vector, i.e. $\mathcal D \subseteq \mathbb R^{n }$.

\begin{definition}
   Let $\mathcal D$ be a dataset containing $m$ elements. The \textit{similarity matrix} $A \in \mathbb{R}^{m \times m}$ associated with $\mathcal D$ is defined as follows: 
   \begin{equation}
      A_{ij} = s\,(d_i,d_j), \text{ for each } (i,j) \in \Iintv{1,m}^2,
   \end{equation}
   where $s$ is a similarity measure and $d_i \in \mathcal D$ for each $i \in \Iintv{1,m}$.
   In this thesis, we will only consider the \textit{Gaussian similarity measure}, which is given by 
   \begin{equation}
      \begin{split}
         s_G:\,\,\,\,\,\,\,\,   E^2  &\longrightarrow \mathbb R  \\
          (x, y) & \longmapsto \exp \left( -\frac{1}{2 \sigma ^2} {\norm{x - y}}^2 \right),
      \end{split}
   \end{equation}
   where $E$ is a normed vector space with norm $\norm{\cdot}$ and $\sigma \in \mathbb R$ is a parameter set by the user which controls the width of the neighborhoods.
\end{definition}

We can think of the similarity matrix above as encoding the \textit{adjacency matrix} of a weighted graph $G_{\mathcal D} = (V_{\mathcal D}, E_{\mathcal D})$ representing the dataset $\mathcal D$. 
In this case, each element $A_{ij}$ of $A$ represents the weight of an edge connecting the vertices $(v_i, v_j) \in {V_{\mathcal D}}^2$.

\begin{remark}
It is convenient here to establish a bijective relationship between the similarity matrix of a dataset and the adjacency matrix of graph.
   In other words, although we have seen that we may obtain a new graph (represented by an adjacency matrix $A$) from a dataset with similarity matrix $A$, we may also obtain a new dataset (represented by a similarity matrix $A$) from a graph with adjacency matrix $A$.
\end{remark}

\begin{definition}
   Let $A \in \mathbb R ^{m \times m}$ be the adjacency matrix of a graph $G = (V, E)$. The \textit{unnormalized Laplacian matrix} of the graph $G$ is defined by
   \begin{equation}
      L_0 = D - A,
   \end{equation}
   where $D \in \mathbb R ^{m \times m}$ is defined to be the diagonal matrix whose $D_{ii}$ elements are given by the sum of the elements of the matrix $A$'s $i$-th row, for all $i \in \Iintv{1,m}$.
\end{definition}

\begin{definition}\label{unnormalized}
   Let $L_0 \in \mathbb R ^{m \times m}$ be the unnormalized Laplacian matrix of a graph $G = (V, E)$. The \textit{normalized Laplacian matrix} of the graph $G$ is defined by
   \begin{equation}
      L = D^{-1/2}L_0D^{-1/2},
   \end{equation}
   where $D \in \mathbb R ^{m \times m}$ is defined to be the diagonal matrix whose $D_{ii}$ elements are given by the sum of the elements of the matrix $A$'s $i$-th row, for all $i \in \Iintv{1,m}$. 
\end{definition}

\section{Ideal case approach}
Here we will consider the ideal case for spectral clustering where, for all $(i,j) \in \Iintv{1,m}^2$, $A_{ij} = 0$ whenever $d_i$ and $d_j$ are in different clusters, and $A_{ij} > 0$ otherwise.
We will only consider the case where the number of clusters $k$ is $3$ and we will assume that, for all $(i,j) \in \Iintv{1,m}^2$, $d_i \in \mathcal D$ are ordered in such a way that $i < j$ whenever the label of $d_i$ is smaller than the label of $d_j$ (remember that the labels $l$ are elements of $\Iintv{1,k}$).
The argument, however, can be easily extended to a general case.

In this section, for algebraic convenience, we will use an alternative definition for the unnormalized Laplacian matrix: $L_{\text{new}} = D^{-1/2}AD^{-1/2}$, instead of $L = D^{-1/2}L_0D^{-1/2}$.
The use of this trick is justified by the fact that, since $L_0 = D - A$, we have that $L_{\text{new}} + L = I_m$, where $I_m$ is the identity matrix of order $m$.
Therefore $L_{\text{new}}$ and $L$ possess the same eigenvectors, and, for each $i \in \Iintv{1,m}$, if $\lambda _i$ is an eigenvalue of $L$, then $1 - \lambda _i$ is an eigenvalue of $L_{\text{new}}$. 
Outside of this section, however, we will use the normal definition of unnormalized Laplacian as given in the previous section.

Before entering the derivation, we need to outline a result.

\begin{proposition}
   \label{bigeigenvalue}
   Let $G$ be a connected graph of order $m$.
   The normalized Laplacian associated with $G$ has the eigenvalue $1$ with positive eigenvector.
   Furthermore, all the other eigenvalues are smaller than $1$.
\end{proposition}

This is a basic result in spectral graph theory.
A proof may be found in, e.g., \cite{mahoney}.

Consider the dataset $\mathcal D$ corresponding to the graph $G$ and let $d_i \in \mathcal D$ for all $i \in \Iintv{1,m}$. Since $A_{ij} = 0$ whenever $d_i$ and $d_j$ are in different clusters, $A \in \mathbb{R} ^{m \times m}$ may be expressed as a block matrix as follows:

\begin{equation}
   A = 
   \begin{bmatrix}
      A^{(1)} & 0_{m_1 \times m_2} & 0_{m_1 \times m_3} \\
      0_{m_2 \times m_1} & A^{(2)} & 0_{m_2 \times m_3} \\
      0_{m_3 \times m_1} & 0_{m_3 \times m_2} & A^{(3)}
   \end{bmatrix}.
\end{equation}

Here, all the elements of the three matrices $A^{(1)} \in \mathbb R^{m_1 \times m_1}$, $A^{(2)} \in \mathbb R^{m_2 \times m_2}$ and $A^{(3)} \in \mathbb R ^{m_3 \times m_3}$ are positive (that is, all their elements are positive) and we have that $m_1+m_2+m_3 = m$. 
From now on in this section, to avoid verbosity, we will omit the subscripts of the $0$ matrices.

It follows from the definitions that the normalized Laplacian $L$ and the diagonal matrix $D$ can be expressed as block matrices in a similar way:

\begin{equation}
 D =
   \begin{bmatrix}
      D^{(1)} & 0 & 0 \\
      0 & D^{(2)} & 0 \\
      0 & 0 & D^{(3)}
   \end{bmatrix}
   \,\,\,\,\,\text{ and }\,\,\,\,\,
   L = 
   \begin{bmatrix}
      L^{(1)} & 0 & 0 \\
      0 & L^{(2)} & 0 \\
      0 & 0 & L^{(3)}
   \end{bmatrix},
\end{equation}

where $D^{(1)} \in \mathbb R ^{m_1 \times m_1}$, $D^{(2)} \in \mathbb R ^{m_2 \times m_2}$ and $D^{(3)} \in \mathbb R ^{m_3 \times m_3}$ are themselves diagonal matrices and $L^{(1)} \in \mathbb R ^{m_1 \times m_1}$, $L^{(2)} \in \mathbb R ^{m_2 \times m_2}$ and $L^{(3)} \in \mathbb R ^{m_3 \times m_3}$ are positive normalized Laplacians of each element of the partition $(\mathcal P_1, \mathcal P_2, \mathcal P_3)$.
Here, the following relation holds for each $i \in \{1, 2, 3 \}$:

\begin{equation}
   L^{(i)} = \left( D^{(i)} \right) ^{-1/2} A^{(i)} \left( D^{(i)} \right) ^{-1/2}.
\end{equation}

Since the matrix $L$ is block-diagonal, its set of eigenvalues $\sigma _L$ is given by the union of the set of eigenvalues of each block $L_1$, $L_2$ and $L_3$, respectively $\sigma_{L_1}$, $\sigma_{L_2}$ and $\sigma_{L_3}$.
Furthermore its eigenvectors are the same as the ones of $L_1$, $L_2$ and $L_3$, provided that they are ``extended'' with $0$ elements as necessary. 
The proof of this claim may also be found in \cite{mahoney}.

By Proposition~\vref{bigeigenvalue}, we know that each $L^{(i)}$ ($i \in \{1, 2, 3 \}$) has $1$ as an eigenvalue with positive eigenvector, which we denote by $x_1^{(i)} \in {\mathbb R_{>0}}^{m_i} $. Furthermore, all other eigenvalues of each $L^{(i)}$ are smaller than $1$.
This implies that $L$ has $1$ as an eigenvalue with multiplicity $3$.
Let $X$ be the matrix containing the eigenvectors associated with these eigenvalues as columns. We have that 

\begin{equation}
   X =
   \begin{bmatrix}
      x_1^{(1)} & 0 & 0 \\
      0 & x_1^{(2)} & 0 \\
      0 & 0 & x_1^{(3)}
   \end{bmatrix}
   \in \mathbb R^{m \times 3}.
\end{equation}

However, from elementary linear algebra, we know that for a Hermitian matrix if $v_1$ and $v_2$ are two eigenvectors associated with a certain eigenvalue, so is $\alpha v_1 + \beta v_2$, for all $(\alpha, \beta) \in \mathbb R ^2$.
Since the normalized Laplacian is Hermitian, we could have picked any other three eigenvectors spanning the same subspace as the ones above.
The actual eigenvectors we obtain may depend on the small perturbations in the normalized Laplacian and the eigensolver used.
This means that we could have gotten $XR$ instead of $X$, for any orthogonal matrix $R \in \mathbb{R}^{3 \times 3}$.
Therefore, we make the transformation $X \longmapsto XR$ to the matrix above in our analysis.

By normalizing the rows of the matrix $X$, we construct the matrix $Y \in \mathbb{R}^{m \times 3}$ as follows:

\begin{equation}\label{y}
   Y = 
   \begin{bmatrix}
      1_{m_1 \times 1} & 0 & 0 \\
      0 & 1_{m_2 \times 1} & 0 \\
      0 & 0 & 1_{m_3 \times 1} 
   \end{bmatrix}R.
\end{equation}

If we let ${R_1}^T \in \mathbb{R}^{1 \times 3}$, ${R_2}^T \in \mathbb{R}^{1 \times 3}$ and ${R_3}^T \in \mathbb{R}^{1 \times 3}$ represent the rows of the matrix $R$, Equation~\vref{y} tells us that the $i$-th row of $Y$ is given by ${R_j} ^T$, where $i \in \Iintv{1,m}$, $j \in \{1, 2, 3 \}$ and $d_i \in \mathcal P_j$ (i.e. the label of the $i$-th data instance is $j$).

As a result, the rows of the matrix $Y$ related to the same label $i$ will cluster in the same point ${R_i}^T$.
Furthermore, from the fact that $R$ is an orthogonal matrix, we deduce that rows of $Y$ corresponding to different labels will cluster in points (located in the unit sphere) that are perpendicular to each other. 
This permits us to use the rows of the matrix $Y$ to easily recover the labels of each $d_i \in \mathcal D$, with $i \in \Iintv{1,m}$, by e.g. applying the k-means algorithm to these rows.

Needless to say, most matrices we deal with are not in the ideal form we assumed they were in this section's discussion.
However, we can think of a general matrix $A$ as being of the norm $A = A_{\text{ideal}} + E$, where $A_{\text{ideal}}$ is a matrix in the ideal form we discussed in this section and $E$ represents the perturbation from the ideal case.
As long as the norm of $E$ is small enough, it is possible to prove that a spectral algorithm based on the approach of this section works.
A more detailed description of the approach used in this section can be found in \cite{ng}.


\section{Relaxation approach}
In this section we will derive the same spectral clustering algorithm as we did in the last section by framing the clustering problem as a discrete optimization problem and relaxing it so it is not discrete anymore.
Before doing that, however, we need to give some definitions and outline some preliminary results.

\subsection{Background}

% Define Cut and NCut
\begin{definition}
   Let $G = (V,E)$ be a graph of order $m$ with adjacency matrix $A \in \mathbb R^{m \times m}$, and let $C$ be a proper subset of the set of vertices $V$. 
   Set $\overline{C} = V \setminus C$.
   The \textit{cut} of the subset $C$ is defined as follows:
   \begin{equation}
      \cut (C) = \sum_{\substack{v_i \in C \\ v_j \notin \overline C }} A_{ij}.
   \end{equation}
   When multiple graphs are under discussion, there are cases in which we write the name of the graph considered as in $\cut _G \,(C)$ to make things clearer.
\end{definition}

The cut of a set of vertices $C$ is a measure of how much the elements of $C$ are connected with the vertices of $\overline C$.
For that reason, it is minimized when $C$ is a separated component. 
One may try, then, to conduct clustering by minimizing the cut of the several elements of a proper partition of $V$.
However, a problem with this idea is that an eventual algorithm trying to achieve this objective might minimize the cut by separating individual vertices from the rest of the graph, which is not what we desire.
A possible approach to deal with this complication is to normalize the cut in such a way that small clusters are ``penalized''.
This leads to the next definition.

\begin{definition}
   Let $G = (V,E)$ be a graph of order $m$ with adjacency matrix $A \in \mathbb R^{m \times m}$, and let $(C_1, C_2, \cdots, C_k)$, where $k \in \Iintv{2,m}$, be a proper partition of the set of vertices $V$.
   The \textit{normalized cut} of the partition $(C_1, C_2, \cdots, C_k)$ is defined as the following quantity:
   \begin{equation}
      \ncut (C_1, C_2, \cdots, C_k) = \sum _{i = 1}^k \frac{\cut (C_i)}{\vol(C_i)}.
   \end{equation}
   Here, $\vol (C_i)$ denotes the sum of the degrees of all the vertices $v \in C_i$ for each $i \in \Iintv{1,k}$.
\end{definition}

With this definition, we can think of the objective of spectral clustering as follows: given a graph $G=(V,E)$, and the number of clusters $k$, we wish to find a proper partition $(C_1, C_2, \cdots, C_k)$ of $V$ such that $\ncut (C_1, C_2, \cdots, C_k)$ is minimized. Unfortunately, this discrete optimization problem cannot be solved efficiently by brute force (more on this later). Therefore we will show how to derive a way of minimizing this quantity for $k = 2$ by relaxation. For the general case, the reader may consult \cite{tutorial}.

In the following proposition, we will show a useful form for expressions of the type $x^TLx$, where $L$ is a Laplacian matrix and $x$ is a real vector.
As we will see later, this will come handy when we want to find relationships between Laplacian matrices and the normalized cut of certain partitions.

\begin{proposition}\label{xtlx}
   Let $G = (V,E)$ be a graph of order $m$ with adjacency matrix $A \in \mathbb R^{m \times m}$, and let $L_0$ be the unnormalized Laplacian matrix associated with $G$. Let $x \in \mathbb R^{m }$ be a real vector. Furthermore, for each $i \in \Iintv{1,m}$, let $d_i$ denote the degree of the vertex $v_i$. Then we have
   \begin{equation}
      x^TL_0x = \frac{1}{2}\sum_{i,j = 1}^m A_{ij} \left( x_i - x_j \right)^2.
   \end{equation}
\end{proposition}

\begin{proof}
   \begin{equation*} 
      \begin{split}
         x^TL_0x &= x^TDx - x^TAx \\
         &= \sum_{i=1}^m d_i{x_i}^2 - \sum_{i,j = 1}^m x_i  x_j A_{ij}  \\
         &= \frac{1}{2} 2 \sum_{i=1}^m \left( \sum_{j=1}^mA_{ij} \right){x_i}^2 - \frac{1}{2}\sum_{i,j = 1}^m 2x_i  x_j A_{ij}  \\
         &= \frac{1}{2} \sum_{i,j=1}^m (x_i^2 +x_j^2 - 2x_ix_j)A_{ij} \\
         &= \frac{1}{2} \sum_{i,j=1}^m A_{ij} \left( x_i - x_j \right) ^2
      \end{split}
   \end{equation*} \qed
\end{proof}

The proposition above allows us to say the following:

\begin{corollary}\label{unnormalizedLaplacianProperties}
   The unnormalized Laplacian $L_0$ of a graph $G$ has the following properties:
   \begin{enumerate}
      \item It is positive semidefinite.
      \item The vector $1_{m \times 1}$ is one of its eigenvectors with corresponding eigenvalue $0$.
      \item Thus its eigenvalues can be written as $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_m = 0$.
   \end{enumerate}
\end{corollary}

And here we prove the generalization of the result above.

\begin{proposition}\label{xtlx2}
   Let $G = (V,E)$ be a graph of order $m$ with no isolated vertices and with similarity matrix $A \in \mathbb R^{m \times m}$, let $L$ be the normalized Laplacian matrix associated with $G$, and let $x \in \mathbb R^{m} $ be a real vector. Furthermore, for each $i \in \Iintv{1,m}$, let $d_i$ denote the degree of the vertex $v_i$. Then we have
   \begin{equation}
      x^TLx = \frac{1}{2}\sum_{i,j = 1}^m A_{ij} \left( \frac{x_i}{\sqrt{d_i}} - \frac{x_j}{\sqrt{d_j}} \right)^2.
   \end{equation}
\end{proposition}

\begin{proof}
   \begin{equation*} 
      \begin{split}
         x^TLx &= x^T D ^{-1/2} L_0 D^{-1/2} x \\
         &= x^Tx - x^TD^{-1/2}AD^{-1/2}x \\
         &= \sum_{i=1}^m {x_i}^2 - \sum_{i,j = 1}^m x_i  x_j \frac{A_{ij}}{\sqrt{d_i d_j}}  \\
         &= \frac{1}{2} \left( \sum_{i=1}^m {x_i}^2 - 2\sum_{i,j = 1}^m \frac{x_i}{\sqrt{d_i}} \frac{x_j}{\sqrt{d_j}} A_{ij} + \sum_{j=1}^m {x_j}^2 \right) \\
         &= \frac{1}{2} \left( \sum_{i=1}^m \left( \frac{x_i}{\sqrt{d_i}} \right) ^2 d_i - 2\sum_{i,j = 1}^m \frac{x_i}{\sqrt{d_i}} \frac{x_j}{\sqrt{d_j}} A_{ij} +  \sum_{j=1}^m \left( \frac{x_j}{\sqrt{d_j}} \right) ^2 d_j \right) \\
         &= \frac{1}{2} \left( \sum_{i=1}^m \left( \frac{x_i}{\sqrt{d_i}} \right) ^2 \left( \sum_{j=1}^m A_{ij} \right) - 2\sum_{i,j = 1}^m \frac{x_i}{\sqrt{d_i}} \frac{x_j}{\sqrt{d_j}} A_{ij} +  \sum_{j=1}^m \left( \frac{x_j}{\sqrt{d_j}} \right) ^2 \left( \sum_{i=1}^m A_{ij} \right) \right) \\
         &= \frac{1}{2} \sum_{i,j=1}^m \left( \left( \frac{x_i}{\sqrt{d_i}} \right)^2 - 2\frac{x_i}{\sqrt{d_i}}\frac{x_j}{\sqrt{d_j}} + \left( \frac{x_j}{\sqrt{d_j}} \right) ^2 \right) A_{ij} \\
         &= \frac{1}{2} \sum_{i,j=1}^m A_{ij} \left( \frac{x_i}{\sqrt{d_i}}- \frac{x_j}{\sqrt{d_j}} \right)^2. \qed
      \end{split}
   \end{equation*}\qed
\end{proof}

The proposition above allows us to say the following:

\begin{corollary}
   The normalized Laplacian $L$ of a graph $G$ has the following properties:
   \begin{enumerate}
      \item It is positive semidefinite.
      \item The vector $D^{1/2} \, 1_{m \times 1}$ is one of its eigenvectors with corresponding eigenvalue $0$.
      \item Thus its eigenvalues can be written as $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_m = 0$.
   \end{enumerate}
\end{corollary}

Finally, before entering the second derivation of spectral clustering proper, we need to state two theorems which relate optimization of expressions of the form $x^TMx$ and eigenvalues.
These theorems are collectively known as \textit{Courant-Fischer Min-Max Theorems}.
   It is worthy to note here that the Propositions~\ref{xtlx}~and~\vref{xtlx2} are also important because, as we will see next, the generalized Courant-Fischer Min-Max Theorem requires that one of the matrices concerned be positive semidefinite. 

\begin{theorem} \label{minmax1}
   Let $m$ denote a positive integer, let $M \in \mathbb{C}^{m \times m}$ be a Hermitian matrix and denote its eigenvalues by $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_m$. 
   Assume $U$ and $V$ denote linear subspaces of $\mathbb C^{m }$.
   Then for all $i \in \Iintv{1,m}$ the following holds:
\begin{equation}
   \lambda_i = \min_{\dim (U) = i} \max_{\substack{x \in U \\ x \ne 0_{m \times 1}}} \frac{x^HMx}{x^Hx} = \max_{\dim (V) = m-i+1} \min_{\substack{x \in V \\ x \ne 0_{m \times 1}}} \frac{x^HMx}{x^Hx}.
\end{equation}
\end{theorem}

\begin{theorem} \label{minmax2}
   Let $m$ denote a positive integer, let $M \in \mathbb {C} ^{m \times m}$ be a Hermitian matrix and $N \in \mathbb C ^{m \times m}$ be a Hermitian positive semidefinite matrix such that $\mathcal N (N)  \subseteq \mathcal N (M)$.
   Assume $U$ and $V$ denote linear subspaces of $\mathbb C^{m }$.
   Denote by $r$ the rank of matrix $M$ and by $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_r$ the generalized eigenvalues of the pencil $(M,N)$.
   Then for all $i \in \Iintv{1,r}$ the following holds:
   \begin{equation}
      \lambda_i = \min_{\substack{ \dim U = i \\ U \bot \mathcal N (N)}} \max_{x \in U} \frac{x^HMx}{x^HNx} = \max_{\substack{\dim  V = r - i + 1 \\ V \bot \mathcal N (N)}} \min_{x \in V} \frac{x^HMx}{x^HNx}.
   \end{equation}
\end{theorem}

A proof of these theorems can be found in \cite{minmax}.

\subsection{Derivation}

Let $m$ and $n$ be positive integers. 
Consider the dataset $\mathcal D \subseteq \mathbb R ^{n }$ and its associated graph $G = (V,E)$ with similarity matrix $A \in \mathbb R^{m \times m}$.
Let $C$ be a proper subset of $V$, and let $D \in \mathbb R ^{m \times m}$ be the diagonal matrix such that $D_{ii}$ is equal to the degree of $v_i \in V$ for all $i \in \Iintv{1,m}$.
Our objective is to set $C$ such that 
\begin{equation}
   \ncut (C, \overline C)
\end{equation}
is minimized.
A proof that this optimization problem is NP-complete may be found at \cite{normalized}. 
Therefore, we need another approach in order to perform clustering in a graph by minimizing the normalized cut.

\begin{definition}
The indicator vector $x_C \in \mathbb R ^{m }$ is defined by:
   \begin{equation} \label{indicator}
   (x_C)_i =
   \begin{cases}
      \sqrt{ \vol  \left( \overline C \right) / \vol \left( C \right) } &\text{ if } v_i \in C \\
      -\sqrt{\vol \left( C \right) / \vol \left( \overline C \right)} & \text{ if } v_i \in \overline C 
   \end{cases}
\end{equation}
for each $i \in \Iintv{1,m}$.
\end{definition}

Our goal here is to find a relationship between ${x_C}^T L x_C$ and the normalized cut of $A$.
Before that, consider the following lemmas:

\begin{lemma} \label{cond1}
   The following holds:
   \begin{equation}
      (D x_C)^T 1_{m \times 1} = 0.
   \end{equation}
\end{lemma}

\begin{proof}
   Let $d_i$ denote the degree of the vertex $v_i$ for each $i \in \Iintv{1,m}$. We have, then:
   \begin{equation*}
      \begin{split}
         (D x_C)^T 1_{m \times 1} &= \sum_{i=1}^m d_i \cdot (x_C)_i \\
         &= \sum_{v_i \in C} d_i \cdot (x_C)_i + \sum_{v_i \in \overline C} d_i \cdot (x_C)_i \\
         &= \sum_{v_i \in C} d_i \sqrt{\vol \left( \overline C \right) / \vol \left( C \right) } - \sum_{v_i \in \overline C} d_i \sqrt{\vol \left( C \right) / \vol \left( \overline C \right) } \\
         &= \vol \left( C \right) \sqrt{\vol \left( \overline C \right) / \vol \left( C \right) } - \vol \left( \overline C \right) \sqrt{\vol \left( C \right) / \vol \left( \overline C \right) } \\
         &= 0. \qed
      \end{split}
   \end{equation*}
\end{proof}

\begin{lemma} \label{cond2}
   The following holds:
   \begin{equation}
      {x_C}^TDx_C = \vol (V).
   \end{equation}
\end{lemma}

\begin{proof}
   As in the lemma above, let $d_i$ denote the degree of the vertex $v_i$ for each $i \in \Iintv{1,m}$. We have:
   \begin{equation*}
      \begin{split}
         {x_C}^T D x_C &= \sum_{i,j=1}^m D_{ij} \cdot (x_C)_i \cdot (x_C)_j \\
         &= \sum_{i=1}^m d_i \cdot {(x_C)_i}^2 \\
         &= \sum_{v_i \in C} d_i \left( \sqrt{\frac{\vol (\overline C)}{\vol (C)}} \right)^2
         + \sum_{v_i \in \overline C} d_i \left( \sqrt{\frac{\vol (C)}{\vol (\overline C)}} \right)^2 \\
         &= \vol(C) \frac{\vol (\overline C)}{\vol(C)} + \vol(\overline C) \frac{\vol (C)}{\vol ( \overline C)} \\
         &= \vol (V). \qed
      \end{split}
   \end{equation*}
\end{proof}

And here, finally, we relate ${x_C}^TL{x_C}$ and the normalized cut.

\begin{theorem}\label{xtxcut}
   The following holds:
   \begin{equation}
      {x_C}^TL_0{x_C} = \vol(V)\, \ncut (C, \overline C).
   \end{equation}
\end{theorem}

\begin{proof}
   We already know that
   \begin{equation*}
         {x_C}^TL_0{x_C} = \frac{1}{2} \sum_{i,j=1}^m A_{ij} ((x_C)_i - (x_C)_j) ^2 .
   \end{equation*}
   Since whenever $(v_i, v_j) \in C^2$ or $(v_i, v_j) \in {\overline C}^2$ (where $(i,j) \in \Iintv{1,m}^2$) we have that $(x_C)_i - (x_C)_j = 0$, we can write ${x_C}^TL_0{x_C}$ as follows:
   \begin{equation*}
      \begin{split}
         {x_C}^TL_0{x_C} &=  \frac{1}{2} \sum_{v_i \in C, v_j \in \overline C}A_{ij} \left( \sqrt{\frac{\vol (\overline C)}{\vol (C)}} + \sqrt{\frac{\vol (C)}{\vol (\overline C)}} \right)^2 + \frac{1}{2} \sum_{v_i \in \overline C, v_j \in C} A_{ij} \left( -\sqrt{\frac{\vol (\overline C)}{\vol (C)}}  -\sqrt{\frac{\vol (C)}{\vol (\overline C)}} \right)^2 \\
         &= \cut (C) \left( \frac{\vol (\overline C)}{\vol (C)} + \frac{\vol (C)}{\vol (\overline C)} + 2 \right) \\
         &= \cut (C) \left( \frac{\vol (C) + \vol (\overline C)}{\vol (C)} + \frac{\vol (C) + \vol (\overline C)}{\vol (\overline C)} \right) \\
         &= \vol (V) \left( \frac{\cut(C)}{\vol (C)} + \frac{\cut(\overline C)}{\vol (\overline C)} \right) \\
         &= \vol (V)\, \ncut(C, \overline C).
      \end{split}
   \end{equation*}

Here we have used the fact that $\cut(C) = \sum_{v_i \in C, v_j \in \overline C} A_{ij} =\sum_{v_i \in \overline C, v_j \in C} A_{ij} = \cut(\overline C)$ . \qed
\end{proof}

Considering that $\vol (V)$ is constant for a given graph, the objective function for clustering,
\begin{equation}
   \min_C \ncut\,(C, \overline C),
\end{equation}
can be expressed as
\begin{equation}\label{npequation}
   \min_{x_C \in \mathbb R^{m }} \,{x_C}^T L_0 x_C \text{,}
\end{equation}
where $x_C$ is defined as in Equation~\vref{indicator}.

As discussed before, this is a NP-complete problem.
To deal with this issue, we may try to relax the condition that $x_C$ is an indicator vector and treat it as a normal vector in $\mathbb R^{m }$.
However, in order not to lose too much information from the optimization constraints, we should also incorporate the two conditions that $x_C$ obeys given by Lemma~\ref{cond1} and Lemma~\vref{cond2} in our new constraint. We get, then:

\begin{equation}
   \min_{x \in \mathbb R^{m }} {x}^TL_0{x}\,\text{ subject to } (Dx)\, \bot \, 1_{m \times 1}  \text{ and } x^T D x = \vol (V).
\end{equation}

In order to put the constraining problem above in the form given by Courant-Fischer Min-Max Theorem, we can make the substitution $y = D^{1/2}x$ and obtain

\begin{equation} \label{optm}
   \min_{y \in \mathbb R^{m }} y^TLy \,\text{ subject to } y \, \bot \, (D^{1/2} \,1_{m \times 1}) \text{ and } y^T y = \vol (V).
\end{equation}

Using Theorem~\vref{minmax1} for $k = 2$ we know that the second biggest eigenvalue of the matrix $L$ satisfies:

\begin{equation}\label{optm2}
   \lambda_2 = \left( 1/\vol(V) \right) \max_{\dim (V) = m-1} \min_{\substack{y \in V \\ y \bot D^{1/2}1_{m \times 1}}} \, y^TLy.
\end{equation}

Furthermore, knowing that $y$ is perpendicular to the eigenvector corresponding to the eigenvalue $\lambda_1 = 0$, the eigenvector corresponding to the second largest eigenvalue of $L$ is the solution to the optimization problem given by Equation~\ref{optm2} and consequently the one given by Equation~\ref{optm}.

Clearly, obtaining $y$ and consequently $x$ does not give us $C$ immediately.
However, we can consider the coordinates of $x \in \mathbb R^{m }$ as points in $\mathbb R$, use $k$-means to cluster them and recover $C$.

\chapter{Spectral Clustering with the Bethe Hessian}


\chapter{Constrained Spectral Clustering with FAST-GE-2.0}



% Introduction
The Information Age has brought with it large incentives to organize and process big amounts of data.
Traditionally, two main approaches have been used to deal with this task: classification and clustering.
While classification is widely used in situations where training data is abundant, such as recommendation systems, spam detection and speech recognition, this class of methods is not applicable to unlabeled datasets, which have been traditionally handled by clustering algorithms.
However, since clustering only makes use of the internal structure of the data, our control over the process is limited.
In this context, a new class of semi-supervised algorithms known as constrained clustering has appeared.
While these methods do not demand large amounts of labeled data as inputs, they still make it possible for a small amount of training data to influence the final outcome of the clustering process.
In this chapter, we describe FAST-GE-2.0, a spectral way of performing constrained clustering and see the theory behind its correctness.
This chapter is mainly a survey of the results from \cite{fastge2}, although we have changed some of the presentation and notation as to make them fit better with the rest of this thesis.

\section{Constrained Clustering}
In this chapter, $m$, $n$, and $k$ represent positive integers, with $k > 1$.

% What is Constrained Clustering? Why is it important? Where is it used?
Given a dataset $\mathcal D \subseteq \mathbb{R}^{n }$ (or equivalently a weighted graph $G = (V,E)$ of order $m$) and a set of contraints, to perform constrained clustering on the data means to find a proper partition $(C_1, C_2, \cdots, C_k)$ of $V$ such that:
\begin{itemize}
   \item For all $i \in \Iintv{1,k}$, edges of vertices in the same subset $C_i$ have big weights.
   \item For all $(i,j) \in \Iintv{1,k}^2$, edges of vertices in different subsets $C_i$ and $C_j$ have small weights.
   \item Constraints are followed as much as possible.
\end{itemize}

These constraints are usually small in number and represent whether certain groups of vertices should forcibly stay together or forcibly stay apart.
For example, in image segmentation, one of the main applications of constrained spectral clustering, a user selects a small amount of points in an image that she believes should stay in the same segment (e.g. points of a uniform background, or points of a tree). Then the contrained clustering algorithm tries to divide the image in segments (clusters) such that the points selected by the user stay in the same segment.


% How are constraints going to be represented in this chapter (FAST-GE-2.0)
There are several ways of representing these constraints, each leading to different algorithms. 
In this thesis we will work with must-link constraints (ML) and cannot-link constraints (CL) encoded as follows:

A set of constraints is given by $k$ disjoint subjects of $V$,
\begin{equation}
   \{ V_1, V_2, \cdots, V_k \} \subseteq V,
\end{equation}
such that: (1) for all $i \in \Iintv{1,k}$, if $(u,v) \in {V_i}^2$ then there exists a ML contraints between the vertices $u$ and $v$; and (2) for all $(i,j) \in \Iintv{1,k}^2$, if $(u,v) \in V_i \times V_j$ and $i \ne j$ then there exists a CL contraint between the vertices $u$ and $v$. 

An algorithm we may eventually develop, then, must be set up in such a way that violations of ML and CL constraints (such as, e.g., two vertices in different constraint sets $V_1$ and $V_2$ being in the same cluster $C_1$) have a negative effect on its effort to satisfy the objective function.

\section{FAST-GE-2.0}
We will now discuss FAST-GE-2.0, a spectral algorithm proposed by Chengming Jiang, et al, for constrained clustering in \cite{fastge2}.
% What is the objective of a p-way contrained clustering (slide; no equation)
We are given a fully connected graph $G = (V,E)$ of order $m$ with adjacency matrix $A$.
Assume a set of constraints $\{V_1, V_2, \cdots, V_k \}$ is given.
The objective of FAST-GE-2.0, in line with our discussion in the last section, is to find a proper partition $(C_1, C_2, \cdots, C_k)$ of $V$ such that $V_i \subseteq C_i$ for all $i \in \Iintv{1,k}$, where, for each pair $(i,j) \in \Iintv{1,m}^2$, vertices $(u,v) \in {C_i}^2$ have edges with high weight and vertices $(u,v) \in {C_i} \times {C_j}$, with $i \ne j$ have edges with low weight.
FAST-GE-2.0 manages to satisfy these constraints indirectly by using auxiliary graphs and encoding the ML and CL constraints into the Laplacian matrices dealth with in the algorithm.

\subsection{Auxiliary graphs}
In this subsection we define the auxiliary graphs used in FAST-GE-2.0.

% Graph G_M (include figure)
\subsubsection*{The graph $G_M$}

\begin{definition}
   The graph $G_M = (V,E)$ is defined by its adjacency matrix 
   \begin{equation}\label{am}
   A_M = \sum_{\ell = 1}^k A_{M_{\ell}},
\end{equation}
   where, for each $(\ell, i, j) \in \Iintv{1,k} \times \Iintv{1, m}^2$, the entries of the submatrix $A_{M_{\ell}} \in \mathbb R^{m \times m}$ are given by:
   \begin{equation}
      (A_{M_{\ell}})_{ij} =
      \begin{cases}
         (d_i d_j) / (d_{\min} d_{\max}) & \text{ if $(v_i, v_j) \in {V_{\ell}}^2$} \\
         0 & \text{ otherwise.}
      \end{cases}
\end{equation}
Here, for each $i \in \Iintv{1,m}$, $d_i$ represents the degree of the vertex $v_i$. 
Futhermore, $d_{\min}$ and $d_{\max}$ represent the smallest and biggest element of the set $\{ d_i \}_{i=1}^m$, respectively.
\end{definition}

As shown in Figure~\vref{gm}, if we define $G_M$ as above, for any given $\ell \in \Iintv{1,k}$, the quantity
\begin{equation}
   \cut_{G_M\,}(C_{\ell}) = \sum_{\substack{v_i \in C_{\ell} \\ v_j \overline{C_{\ell}}}} (A_M)_{ij} 
\end{equation}
%measures the amount of violations of ML constraints relative to the cut of $C_{\ell}$.
measures the degree to which the proper partition $(C_1, C_2, \cdots, C_k)$ violates the ML constraints.
Therefore we must try to minimize it as much as possible.

\begin{figure}
\begin{center}
\includegraphics[width=8cm]{figures/gm.png}
\end{center}
   \caption[Graphical representation of $\cut_{G_M\,}(C_1)$ for a simple graph $G=(V,E)$]{Graphical representation of $\cut _{G_M\,}(C_1)$ for a simple simple graph $G = (V,E)$.  In this example, red vertices are elements of $V_1$, blue vertices are elements of $V_2$ and black vertices are elements of $V \setminus (V_1 \cup V_2)$. The black lines represent elements of $A_M$ that contribute to the amount of violations of ML given by $\cut _{G_M\,}(C_1)$. Note that all non-zero elements of $A_M$ must connect vertices of the same color, and all terms contributing to $\cut _{G_M\,}(C_1)$ must connect vertices in different clusters; hence the lines in the figure. We want elements of the same color to stay in the same cluster as much as possible. Therefore, we must try to decrease the amount of black lines.}
\label{gm}
\end{figure}

% Graph G_H (include figure)
\subsubsection*{The graph $G_H$}

\begin{definition}
The graph $G_H = (V,E)$ is defined by its adjacency matrix
   \begin{equation}\label{ah}
      A_H = \frac{1}{m} \, (A_C + {A_C}^T + A_K).
   \end{equation}
   Here, $A_C \in \mathbb{R}^{m \times m}$ is a matrix whose values are given by
   \begin{equation}
      (A_C)_{ij} = 
      \begin{cases}
      (d_i d_j)/(d_{\min} d_{\max}) & \text{ if $(v_i, v_j) \in V_{\ell_1} \times V_{\ell_2}$ and $\ell_1 \ne \ell_2$} \\
         0 & \text{ otherwise,}
      \end{cases}
   \end{equation}
   for each $(\ell_1, \ell_2, i, j) \in \Iintv{1,k}^2 \times \Iintv{1,m}^2$. 
   For all $i \in \Iintv{1,m}$, $d_i$ represents the degree of the vertex $v_i$. $d_{\min}$ and $d_{\max}$ represent the smallest and biggest element of the set $\{ d_i \}_{i=1}^m$, respectively.
   Furthermore, $A_K \in \mathbb{R}^{m \times m}$ is a matrix whose entries are given by 
   \begin{equation}
      (A_K)_{ij} = \frac{  \left( d^{(K)} \right) _i \cdot \left( d^{(K)} \right) _j }{ \sum_{p = 1}^m (d^{(K)})_p },
   \end{equation}
   for each $(i,j) \in \Iintv{1,m}^2$. Here, for every $i \in \Iintv{1,m}$, $\left( d^{(K)} \right) _i$ represents the sum of the elements in the $i$-th column of the matrix $A_C + {A_C}^T$.
\end{definition}

As shown in Figure \vref{gh}, if we define $G_H$ as above, for any given $\ell \in \Iintv{1,k}$, the quantity
\begin{equation}
   \cut _{G_H \,} (C_{\ell}) = \sum_{\substack{v_i \in C_{\ell} \\ v_j \in \overline{C_{\ell}}}} (A_H)_{ij} 
\end{equation}
measures the degree to which the proper partition $(C_1, C_2, \cdots, C_k)$ satisfies the CL constraints (as long as we do not consider $A_K$). 
Therefore we must try to maximize it as much as possible.

\begin{figure}
\begin{center}
\includegraphics[width=8cm]{figures/gh.png}
\end{center}
   \caption[Graphical representation of $\cut_{G_H\,}(C_1)$ for a simple graph $G=(V,E)$]{Graphical representation of $\cut _{G_H\,}(C_1)$ for a simple simple graph $G = (V,E)$.  In this example, red vertices are elements of $V_1$, blue vertices are elements of $V_2$ and black vertices are elements of $V \setminus (V_1 \cup V_2)$. The black lines represent elements of $A_C$ that contribute to the amount obedience of CL given by $\cut _{G_H\,}(C_1)$. Note that all non-zero elements of $A_C$ must connect vertices of different colors, and all terms contributing to $\cut _{G_H\,}(C_1)$ must connect vertices in different clusters; hence the lines in the figure. We want elements of different colors to stay in the different clusters as much as possible. Therefore, we must try to increase the amount of black lines.}
\label{gh}
\end{figure}

The matrix $A_K$ is called a \textit{demand matrix}, and it is used in the construction of the graph $G_H$ in order to obtain some guarantees related to the spectral relaxation of FAST-GE-2.0. 
The mathematical details behind its use are beyond the level of this thesis.
More details about it can be found in \cite{fastge1}.


\subsection{Objective function}
In this and the following subsections, we assume $\ell \in \Iintv{1,k}$.

% Measure of badness
From our discussions on the last section, we know that we want to both minimize $\cut _{G_M\,}(C_{\ell})$ and to maximize $\cut _{G_H\,}(C_{\ell})$.
A natural next step, then, is to create some form of measure involving both cuts that we can optimize.

\begin{definition}
  We define the measure of badness $\phi_\ell$ relative to a cluster $C_\ell$ as follows:
   \begin{equation}
      \phi_\ell = \frac{\cut_{G_M\,} (C_\ell) + \cut_{G\,}(C_\ell)}{\cut_{G_H\,}(C_\ell)},
   \end{equation}
   where $G$ is the original graph we are trying to cluster with adjacency matrix $A$.
\end{definition}

Note that from our discussion in the past section, the only way to minimize $\phi_\ell$ is to either
\begin{enumerate}[(a)]
   \item minimize $\cut_{G_M\,}(C_\ell)$, which is the same as minimizing the amount of violations of ML constraints; or to
   \item minimize $\cut_{G\,}(C_\ell)$, which is the same as selecting a better cluster $C_\ell$ from the point of view of pure clustering; or to
   \item maximize $\cut_{G_L\,}(C_\ell)$, which is the same as maximizing the amount of obedience to CL constraints.
\end{enumerate}

Therefore, for any given cluster $C_\ell$, the measure $\phi_\ell$ successfully encapsulates all of our objectives in constrained clustering.

\begin{remark}
   The value $\cut_{G_M\,}(C_\ell) + \cut_{G\,}(C_\ell)$ may be expressed equivalently by $\cut_{G_N\,}(C_\ell)$, where $G_N$ is a \textit{new} graph defined by its adjacency matrix:
   \begin{equation}
      A_N = A_M+A.
   \end{equation}
   We can then write the measure of badness $\phi_\ell$ as
   \begin{equation}
      \phi_\ell = \frac{\cut_{G_N\,}(C_\ell)}{\cut_{G_H\,}(C_\ell)}.
   \end{equation}
\end{remark}

% Get the equation for k-way constrained partioning
We can then set our final objective function as the follows:

\begin{definition}
   The \textit{objective of FAST-GE-2.0} for a $k$-way constrained partitioning is given by
   \begin{equation}\label{objective}
      \min_{(C_1, C_2, \cdots, C_k)} \max_{\ell} \, \phi _\ell.
   \end{equation}
   In other words, we want to find a proper partition $(C_1, C_2, \cdots, C_k)$ of $V$ that minimizes the biggest value of $\phi_\ell$ for all clusters $C_\ell$.
\end{definition}

\subsection{Eigenproblem formulation}
In this subsection, we will analyze the case where $k = 2$ as it was done in \cite{fastge2}.
An analysis of the general case, which may be found in \cite{fastge1}, requires linear algebra knowledge not expected from the main audience of this thesis, so we omit it.

% Refer to theorem to say the equation above is the same as Rayleigh quotient
In the case $k=2$, if we set $C_1 = C$ and $C_2 = \overline C$, the objective function given in Equation \vref{objective} can be rewritten as 
\begin{equation}\label{phi2}
   \min_C \frac{\cut_{G_N\,}(C)}{\cut_{G_H\,}(C)}.
\end{equation}

Here, Theorem \vref{xtxcut} allows us to rewrite Equation \vref{phi2} in more convenient terms. 
An important point to note, however, is that since the $\vol (V)$ is constant, we can ignore it in the optimization analysis.
The objective function becomes then:
\begin{equation}
   \min_{{x_C}^TL_H {x_C} \ne 0} \frac{{x_C}^TL_N{x_C}}{{x_C}^TL_H{x_C}},
\end{equation}
where $L_N$ and $L_H$ are respectively the unnormalized Laplacians of the graphs $G_N$ and $G_H$, and where $x_C$ is an indicator vector as defined in Equation~\vref{indicator}.

% Problem is NP-complete (reference to somewhere that proves it --> check paper)
With a similar argument as the one used for Equation~\vref{npequation}, one can prove that the optimization problem above is NP-complete \cite{fastge2}.
% Approximation
It stands to reason then to perform spectral relaxation and try to apply the General Courant-Fischer Min-Max Theorem to the objective function as we did for regular spectral clustering. The function becomes:
\begin{equation}\label{inf}
   \inf_{\substack{x \in \mathbb R^m \\ x^T L_H x \ne 0}} \frac{x^T L_N x}{x^T L_H x},
\end{equation}
where $x \in \mathbb R^{m }$ is now a arbitrary real column-vector.
Note that we have written $\inf$ instead of $\min$.
The reason for this is that the minimum is not guaranteed to be achieved.

Even after performing the spectral relaxation above, however, we still cannot be sure we are allowed to apply General Courant-Fischer, since it requires not only that the Laplacian $L_H$ in the denominator be positive semidefinite (which is true by Corollary \vref{unnormalizedLaplacianProperties}), but also that $\mathcal N (L_H) \subseteq \mathcal N (L_N)$. 
Let us check whether this condition holds or not.


\begin{proposition} \label{spanOfLn}
   If $G$ is a connected graph, then 
   \begin{equation}
      \mathcal N (L_N) = \spn \{ 1_{m \times 1} \}.
   \end{equation}
\end{proposition}

\begin{proof}
   We known from the definition that $A_N = A + A_M$.
   Since $G$ is connected, all elements of $A$ are positive, and we can conclude that all elements of $A_N$ are also positive.
   Now assume $x \in \mathbb R ^{m}$ is an element of the nullspace of $L_N$, i.e. $L_N x = 0$.
   By Proposition \vref{xtlx}, we know that:
   \begin{equation*}
      x^T (L_N x) = \frac{1}{2} \sum_{i,j=1}^m (A_N)_{ij} (x_i - x_j)^2 = 0.
   \end{equation*}
   For all $(i,j) \in \Iintv{1,m}^2$, $(A_N)_{ij} > 0$, so we must have $x_i - x_j = 0$ for all these pairs.
   That is, all elements of $x$ must necessarily be the same. \qed
\end{proof}

\begin{proposition} \label{spanOfLh}
   Even if $G$ is connected, 
   \begin{equation}
      \mathcal N (L_H) \subseteq \mathcal N (L_N)
   \end{equation}
   does not necessarily hold.
\end{proposition}

\begin{proof}
   We will demonstrate this proposition by showing a connected graph $G$ for which $L_H$ has an eigenvector $x$ such that $x \notin \mathcal \spn \{ 1_{m \times 1} \}$. 
   Assume $G$ is the completely connected graph of order $m$ where $A_{ij} = 1$, for all $(i,j) \in \Iintv{1,m}^2$.
   Assume further that $V_1 = \{ v_1 \}$ and $V_2 = \{ v_2 \}$.
   We must have that $d_i = m(m-1)/2$, for all $i \in \Iintv{1,m}$, and thus, from the definitions given in this section:
   \begin{equation*}
      A_C = 
      \begin{bmatrix}
         0 & 1 & 0 & \cdots & 0 \\
         1 & 0 & 0 & \cdots & 0 \\
         0 & 0 & 0 & \cdots & 0 \\
         \vdots & \vdots & \vdots & \ddots & 0 \\
         0 & 0 & 0 & \cdots & 0 
      \end{bmatrix}.
   \end{equation*}
   We must have then that $\left( d^{(K)} \right) _1 = \left( d^{(K)} \right)_2 = 2$ and $\left( d^{(K)} \right)_i = 0$ for all $i \in \Iintv{3,m}$. Thus
   \begin{equation*}
      A_K =
      \begin{bmatrix}
         1 & 1 & 0 & \cdots & 0 \\
         1 & 1 & 0 & \cdots & 0 \\
         0 & 0 & 0 & \cdots & 0 \\
         \vdots & \vdots & \vdots & \ddots & 0 \\
         0 & 0 & 0 & \cdots & 0 
      \end{bmatrix},
      \text{ and }
      A_H = \frac{1}{m} \, (A_C + {A_C}^T + A_K) = 
      \begin{bmatrix}
         1/m & 3/m & 0 & \cdots & 0 \\
         3/m & 1/m & 0 & \cdots & 0 \\
         0 & 0 & 0 & \cdots & 0 \\
         \vdots & \vdots & \vdots & \ddots & 0 \\
         0 & 0 & 0 & \cdots & 0 
      \end{bmatrix}.
   \end{equation*}
   Finally, computing the Laplacian $L_H = D_H - A_H$, where $D_H = \text{diag} \,(4/m, 4/m, 0 \cdots, 0) \in \mathbb R^{m \times m}$ is the diagonal matrix of the graph:
   $G_H$:
   \begin{equation*}
      L_H =
      \begin{bmatrix}
         3/m & -3/m & 0 & \cdots & 0 \\
         -3/m & 3/m & 0 & \cdots & 0 \\
         0 & 0 & 0 & \cdots & 0 \\
         \vdots & \vdots & \vdots & \ddots & 0 \\
         0 & 0 & 0 & \cdots & 0 
      \end{bmatrix},
   \end{equation*}
   which clearly has $x = {\begin{bmatrix} 0 & 0 & 1 & 1 & \cdots & 1 \end{bmatrix}}^T \in \mathbb R ^{m} \setminus \spn \{ 1_{m \times 1} \}$ as one of its eigenvectors. \qed
\end{proof}

Proposition \vref{spanOfLh} shows to us then that we cannot use the Generalized Courant-Fischer Theorem as a guarantee that the spectral relaxation will work.
Fortunately, Chengming Jiang et al. proved the following theorem in \cite{fastge2}.

\begin{theorem}
   \label{fastge2theorem}
   For the matrices $L_N$ and $L_H$ defined in this chapter, the following holds
   \begin{enumerate}[(a)]
      \item the pencil $(L_N, L_H)$ has $r$ finite non-negative generalized eigenvalues $0 \le \lambda_1 \le \lambda_2 \le \cdots \le \lambda_r$, where $r$ denotes the rank of the matrix $L_H$.
      \item For every $i \in \Iintv{1,r}$, the following holds
         \begin{equation}
            \lambda_i = \max_{\substack{\mathcal X \subseteq \mathbb R^m \\ \dim {\mathcal X} = n-i+1 }} \min_{\substack{x \in \mathcal X \\ x^T L_H x > 0}} \frac{x^T L_N x}{x^T L_H x}.
         \end{equation}
         In particular, 
         \begin{equation}
            \lambda_1 = \min_{\substack{x \in \mathbb R^m \\ x^TL_Hx>0}} \frac{x^tL_N x}{x^T L_H x}.
         \end{equation}
   \end{enumerate}
\end{theorem}

Item (b) of Theorem \vref{fastge2theorem} guarantees to us that the $\inf$ in Equation \vref{inf} can be substituted by a $\min$:
\begin{equation}\label{min}
   \min_{\substack{x \in \mathbb R^m \\ x^T L_H x \ne 0}} \frac{x^T L_N x}{x^T L_H x},
\end{equation}
and that this minimum is given by the smallest finite eigenvalue of the following generalized eigenvalue problem:
\begin{equation}\label{generalizedEigenproblem}
   L_N x = \lambda L_H x.
\end{equation}

Our problem, then, is reduced to solving the generalized eigenproblem given by Equation \vref{generalizedEigenproblem}, a particular case of the well-studied Generalized Hermitian Eigenvalue problem.
Several approaches exist to solve it: Direct methods, Lanczos methods and Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) algorithm, for example. 
However, all of them require that the pencil $(L_N, L_H)$ be non-singular (see \cite{templates}).
In other words, we need the following condition to hold for all $\lambda \in \mathbb R$:
\begin{equation}
   \det (L_N - \lambda L_H) \ne 0,
\end{equation}
which is problematic due to the following proposition:

\begin{proposition}
   The pencil $(L_N, L_H)$ is singular.
\end{proposition}

\begin{proof}
   Since $1_{m \times 1}$ is in the nullspace of both $L_N$ and $L_H$ (Corollary \vref{unnormalizedLaplacianProperties}), we know that, for any $\lambda \in \mathbb R$,
   \begin{equation*}
      (L_N - \lambda L_H) 1_{m \times 1} = L_N 1_{m \times 1} - \lambda L_H 1_{m \times 1} = 0 - \lambda 0 = 0 = 0 \, 1_{m \times 1}.
   \end{equation*}
   Therefore, the matrices $(L_N - \lambda L_H)$ have $0$ as an eigenvalue and are, therefore, singular. \qed
\end{proof}

% Regularization (theorem 2 from paper)
To fix this problem, then, we need to regularize the pencil $(L_N, L_H)$.
The following theorem, also proved in \cite{fastge2}, allows us to do that:

\begin{theorem}
   Suppose the pencil $(L_N, L_H)$ has the finite eigenvalues $\lambda_1 \le \lambda_2 \le \cdots \le \lambda_r$, where $r$ is the rank of the matrix $L_H$. Let
   \begin{equation}\label{regularization}
      K = - L_H,  \text{ and } M = L_N + \mu L_H + ZSZ^T,
   \end{equation}
   where $Z \in \mathbb R^{m \times s}$ is an orthonormal basis of the common nullspace of $L_N$ and $L_H$, $S \in \mathbb R ^{s \times s}$ is an arbitrary positive definite matrix, and $\mu \in \mathbb R$.
   Then the following holds:
   \begin{enumerate}[(a)]
      \item the matrix $M$ is positive definite.
      \item the generalized eigenvalues of the pencil $(K, M)$ are $\sigma _1 \le \sigma_2 \le \cdots \le \sigma_r < \sigma_{r+1} = \sigma_{r+2} = \cdots = \sigma_m = 0$, where, for each $i \in \Iintv{1,r}$, $\sigma_i = -1/(\lambda_i + \mu)$.
   \end{enumerate}
\end{theorem}

The theorem above lets us compute the $k$ smallest eigenvalues $\{ \lambda_i \}_{i=1}^k$ of the generalized eigenproblem in Equation \vref{min} by computing the $k$ \textit{largest} eigenvalues of the following generalized eigenproblem:
\begin{equation}
K x = \sigma M x,
\end{equation}
which can effectively be solve by methods such as Lanczos and LOBPCG.

% Algorithm
Given the considerations above, we can write the spectral algorithm for constrained clustering FAST-GE-2.0 as follows:

\begin{algorithm}
\caption{FAST-GE-2.0}\label{fastge2alg}
\begin{algorithmic}[1]
   \Require 
      \Statex Number of Clusters: $k \in \mathbb Z_{>1}$
      \Statex Adjacency Matrix of the graph $G=(V,E)$: $A \in \mathbb R ^ {m \times m}$ 
      \Statex Constraint Sets: $\{ V_1, V_2, \cdots, V_k \} \subseteq V$
      \Statex Regularization Parameters: $\mu \in \mathbb R$, $Z \in \mathbb R ^{n \times s}$, $S \in \mathbb R ^{s \times s}$
   \Ensure 
      \Statex Partition of the set of vertices $V$: $\{ C_1, C_2, \cdots, C_k \} \subseteq V$
      \vspace{0.2 cm}

   \State Compute the graphs $G_M$ and $G_H$ with respective adjacency matrices $A_M$ and $A_H$ as indicated in Equation \vref{am} and Equation \vref{ah}.
   \State Compute the unnormalized Laplacians $L_N$ and $L_H$ of the graphs $G_N$ and $G_H$. Here the adjacency matrix of $G_N$ is given by $A + A_M$.
   \State Compute $k$ eigenvectors corresponding to the $k$ largest finite generalized eigenvalues of the pencil $(K,M)$ in Equation \vref{regularization}. Let $X \in \mathbb R ^{m \times k}$ be the matrix containing these eigenvectors as columns.
   \State Let $Y \in \mathbb R ^{m \times k}$ be the matrix $X$ with rows and columns normalized.
   \State Let $(y_1, y_2, \cdots, y_k) \in (\mathbb R^{1 \times k})^m$ represent the row-vectors of $Y$.
   \State Cluster $(y_1, y_2, \cdots, y_m)$ using $k$-means into the clusters $ \{ D_1, D_2, \cdots, D_k \}$.
   \State For each $i \in \Iintv{1,k}$, set $C_i = \{ v_j \in V: y_j \in D_i \}$.
\end{algorithmic}
\end{algorithm}

% (maybe) reference to future numerical experiment


\chapter{Proposed Method}


\chapter{Numerical Experiments}
\section{Clustering Evaluation}
\subsection{Normalized Mutual Information}

\section{Experiment 1: }


\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{\numberline{}Acknowledgements}

\newpage

\addcontentsline{toc}{chapter}{\numberline{}References}
\renewcommand{\bibname}{References}

%% 参考文献に jbibtex を使う場合
\bibliographystyle{plain}
\bibliography{report}{}
%% [compile] jbibtex sample; platex sample; platex sample;


\end{document}
